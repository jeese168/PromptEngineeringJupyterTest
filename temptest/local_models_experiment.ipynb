{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# æœ¬åœ°æ¨¡å‹ Context Rot å®éªŒ (Colabç‰ˆ)\n",
    "\n",
    "ä½¿ç”¨ Google Colab T4 GPU ç¯å¢ƒéƒ¨ç½² Ollama æœ¬åœ°æ¨¡å‹æ¥æµ‹è¯•ä¸Šä¸‹æ–‡è¡°å‡ç°è±¡\n",
    "\n",
    "**ç¯å¢ƒé…ç½®:**\n",
    "- å¹³å°: Google Colab (T4 GPU)\n",
    "- æœåŠ¡åœ°å€: localhost:11434\n",
    "- æµ‹è¯•æ¨¡å‹: qwen3:0.6b (522MB), qwen3:1.7b (1.4GB), qwen3:8b (8.7GB)\n",
    "- å¯¹æ¯”åŸºå‡†: ä¸åŒå‚æ•°è§„æ¨¡æ¨¡å‹çš„ä¸Šä¸‹æ–‡ä¿æŒèƒ½åŠ›\n",
    "\n",
    "**å®éªŒç›®æ ‡:** å¯¹æ¯” Qwen3 ç³»åˆ—ä¸åŒè§„æ¨¡æ¨¡å‹(0.6B/1.7B/8B)çš„ä¸Šä¸‹æ–‡ä¿æŒèƒ½åŠ›ï¼Œè§‚å¯Ÿ Context Rot ç°è±¡éšå‚æ•°è§„æ¨¡çš„å˜åŒ–è¶‹åŠ¿"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Colab ç¯å¢ƒå¿«é€Ÿä¸Šæ‰‹\n",
    "1. å…ˆåœ¨ Colab è¿è¡Œä¸‹é¢çš„â€œå®‰è£…å¹¶å¯åŠ¨ Ollamaâ€å•å…ƒï¼Œè®©æœ¬åœ°æœåŠ¡èµ·åœ¨ `localhost:11434`ã€‚\n",
    "2. å†è¿è¡Œâ€œä¸‹è½½æ¨¡å‹â€å•å…ƒï¼Œåˆ©ç”¨ Colab å¸¦å®½å¿«é€Ÿæ‹‰å– `qwen3` ç³»åˆ—æ¨¡å‹ã€‚\n",
    "3. å®Œæˆåå³å¯ç›´æ¥æ‰§è¡ŒåŸæ¥çš„å®éªŒä»£ç ï¼Œé»˜è®¤ä¼šæŒ‡å‘æœ¬åœ°çš„ Ollamaã€‚å¦‚æœæƒ³æ”¹å›è¿œç¨‹æœåŠ¡å™¨ï¼Œåªéœ€è¦è®¾ç½®ç¯å¢ƒå˜é‡ `OLLAMA_URL` å³å¯ã€‚\n",
    "4. è‹¥éœ€è¦åœæ­¢åå°æœåŠ¡ï¼Œå¯åœ¨å®‰è£…å•å…ƒé‡Œæš´éœ²çš„ `ollama_server` å¯¹è±¡ä¸Šè°ƒç”¨ `ollama_server.terminate()`ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "â¬‡ï¸ æ­£åœ¨å®‰è£… Ollama...\n",
      "âœ… Ollama å®‰è£…å®Œæˆ\n",
      "ğŸš€ æ­£åœ¨åå°å¯åŠ¨ Ollama æœåŠ¡...\n",
      "âœ… Ollama æœåŠ¡å·²åœ¨ localhost:11434 è¿è¡Œï¼Œå˜é‡ ollama_server å¯ç”¨äºæ‰‹åŠ¨ç»ˆæ­¢\n"
     ]
    }
   ],
   "source": [
    "# === 1. åœ¨ Colab å®‰è£…å¹¶å¯åŠ¨ Ollama ===\n",
    "import subprocess\n",
    "import time\n",
    "import os\n",
    "\n",
    "print(\"â¬‡ï¸ æ­£åœ¨å®‰è£… Ollama...\")\n",
    "install_cmd = \"curl -fsSL https://ollama.com/install.sh | sh\"\n",
    "install_result = subprocess.run(install_cmd, shell=True, check=False)\n",
    "if install_result.returncode == 0:\n",
    "    print(\"âœ… Ollama å®‰è£…å®Œæˆ\")\n",
    "else:\n",
    "    raise RuntimeError(\"âŒ Ollama å®‰è£…å¤±è´¥ï¼Œè¯·æ£€æŸ¥ç½‘ç»œæˆ–ç¨åå†è¯•\")\n",
    "\n",
    "print(\"ğŸš€ æ­£åœ¨åå°å¯åŠ¨ Ollama æœåŠ¡...\")\n",
    "# å°†è¿›ç¨‹å¥æŸ„æš´éœ²åœ¨å…¨å±€ä½œç”¨åŸŸï¼Œæ–¹ä¾¿åç»­æ‰‹åŠ¨åœæ­¢\n",
    "ollama_server = subprocess.Popen([\"ollama\", \"serve\"], stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
    "time.sleep(5)\n",
    "print(\"âœ… Ollama æœåŠ¡å·²åœ¨ localhost:11434 è¿è¡Œï¼Œå˜é‡ ollama_server å¯ç”¨äºæ‰‹åŠ¨ç»ˆæ­¢\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "â¬‡ï¸ æ­£åœ¨ä¸‹è½½ qwen3:0.6b...\n",
      "â¬‡ï¸ æ­£åœ¨ä¸‹è½½ qwen3:1.7b...\n",
      "â¬‡ï¸ æ­£åœ¨ä¸‹è½½ qwen3:8b...\n",
      "â¬‡ï¸ æ­£åœ¨ä¸‹è½½ qwen3:14b...\n",
      "âœ… æ¨¡å‹ä¸‹è½½å®Œæˆï¼Œå¯ä»¥è¿è¡Œåç»­å®éªŒ\n"
     ]
    }
   ],
   "source": [
    "# === 2. ä¸‹è½½æ‰€éœ€æ¨¡å‹ï¼ˆå¯æ ¹æ®éœ€è¦å¢å‡ï¼‰ ===\n",
    "import subprocess\n",
    "\n",
    "print(\"â¬‡ï¸ æ­£åœ¨ä¸‹è½½ qwen3:0.6b...\")\n",
    "pull_small = subprocess.run([\"ollama\", \"pull\", \"qwen3:0.6b\"], check=False)\n",
    "if pull_small.returncode != 0:\n",
    "    raise RuntimeError(\"âŒ qwen3:0.6b ä¸‹è½½å¤±è´¥ï¼Œè¯·ç¡®è®¤ç½‘ç»œæˆ–ç¨åå†è¯•\")\n",
    "\n",
    "# å¦‚æœéœ€è¦æ›´å¤§æ¨¡å‹ï¼Œå¯è§£é™¤ä¸‹ä¸€æ®µæ³¨é‡Š\n",
    "print(\"â¬‡ï¸ æ­£åœ¨ä¸‹è½½ qwen3:1.7b...\")\n",
    "pull_large = subprocess.run([\"ollama\", \"pull\", \"qwen3:1.7b\"], check=False)\n",
    "if pull_large.returncode != 0:\n",
    "    raise RuntimeError(\"âŒ qwen3:1.7b ä¸‹è½½å¤±è´¥\")\n",
    "\n",
    "print(\"â¬‡ï¸ æ­£åœ¨ä¸‹è½½ qwen3:8b...\")\n",
    "pull_qwen_8b = subprocess.run([\"ollama\", \"pull\", \"qwen3:8b\"], check=False)\n",
    "if pull_qwen_8b.returncode != 0:\n",
    "    raise RuntimeError(\"âŒ qwen3:8b ä¸‹è½½å¤±è´¥\")\n",
    "\n",
    "print(\"â¬‡ï¸ æ­£åœ¨ä¸‹è½½ qwen3:14b...\")\n",
    "pull_qwen_14b = subprocess.run([\"ollama\", \"pull\", \"qwen3:14b\"], check=False)\n",
    "if pull_qwen_14b.returncode != 0:\n",
    "    raise RuntimeError(\"âŒ qwen3:14b ä¸‹è½½å¤±è´¥\")\n",
    "\n",
    "print(\"âœ… æ¨¡å‹ä¸‹è½½å®Œæˆï¼Œå¯ä»¥è¿è¡Œåç»­å®éªŒ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ’» æ“ä½œç³»ç»Ÿå†…æ ¸:\n",
      "Linux c8ebd5c9a13d 6.6.105+ #1 SMP Thu Oct  2 10:42:05 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux\n",
      "\n",
      "ğŸš€ æ˜¾å¡ä¿¡æ¯:\n",
      "Sat Nov 22 11:39:51 2025       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  Tesla T4                       Off |   00000000:00:04.0 Off |                    0 |\n",
      "| N/A   52C    P8             11W /   70W |       0MiB /  15360MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|  No running processes found                                                             |\n",
      "+-----------------------------------------------------------------------------------------+\n",
      "\n",
      "ğŸ’€ [ç³»ç»Ÿæ—¥å¿—] æ£€æŸ¥æœ€è¿‘çš„ OOM è®°å½•:\n",
      "nohup: appending output to 'nohup.out'\n"
     ]
    }
   ],
   "source": [
    "print(\"ğŸ’» æ“ä½œç³»ç»Ÿå†…æ ¸:\")\n",
    "!uname -a\n",
    "print(\"\\nğŸš€ æ˜¾å¡ä¿¡æ¯:\")\n",
    "!nvidia-smi\n",
    "print(\"\\nğŸ’€ [ç³»ç»Ÿæ—¥å¿—] æ£€æŸ¥æœ€è¿‘çš„ OOM è®°å½•:\")\n",
    "os.system(\"dmesg | grep -i 'kill' | tail -n 5\")\n",
    "!nohup ollama serve &"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-20T08:03:29.531344Z",
     "start_time": "2025-11-20T08:03:29.523827Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "import json\n",
    "import time\n",
    "\n",
    "# Ollama API é…ç½®\n",
    "DEFAULT_OLLAMA_URL = \"http://127.0.0.1:11434\"\n",
    "OLLAMA_URL = os.getenv(\"OLLAMA_URL\", DEFAULT_OLLAMA_URL)\n",
    "print(f\"ğŸ“¡ å½“å‰ Ollama åœ°å€: {OLLAMA_URL}\")\n",
    "\n",
    "\n",
    "def ollama_chat_qwen(messages, temperature=0.7, max_retries=3, model=\"qwen3:0.6b\", \n",
    "                     debug=False, show_thinking=False):\n",
    "    \"\"\"\n",
    "    è°ƒç”¨ Qwen3 æ¨¡å‹è¿›è¡Œæ¨ç†ï¼ˆçœŸæ­£çš„æµå¼æ˜¾ç¤ºï¼‰\n",
    "    \n",
    "    å‚æ•°:\n",
    "        messages: å¯¹è¯æ¶ˆæ¯åˆ—è¡¨\n",
    "        temperature: æ¸©åº¦å‚æ•° (0.0-1.0)\n",
    "        max_retries: æœ€å¤§é‡è¯•æ¬¡æ•°\n",
    "        model: æ¨¡å‹åç§°\n",
    "        debug: æ˜¯å¦æ˜¾ç¤ºè°ƒè¯•ä¿¡æ¯\n",
    "        show_thinking: æ˜¯å¦æ˜¾ç¤ºæ€è€ƒè¿‡ç¨‹\n",
    "    \n",
    "    æ³¨æ„: \n",
    "        - æ—  timeout é™åˆ¶ï¼ˆæœ¬åœ°æ¨ç†ï¼‰\n",
    "        - num_ctx=8192, num_predict=4096 (å¹³è¡¡æ˜¾å­˜ä¸æ€§èƒ½)\n",
    "        - çœŸæ­£çš„æµå¼æ˜¾ç¤ºï¼Œå®æ—¶æ‰“å°æ¯ä¸ª token\n",
    "    \"\"\"\n",
    "    url = f\"{OLLAMA_URL}/api/chat\"\n",
    "    \n",
    "    # ä¼˜åŒ–åçš„å‚æ•°é…ç½®\n",
    "    options = {\n",
    "        \"temperature\": temperature,\n",
    "        \"top_p\": 0.9,\n",
    "        \"repeat_penalty\": 1.07,  # é™ä½æƒ©ç½šç‡ï¼Œé¿å…è¿‡åº¦å›é¿é‡å¤\n",
    "        \"num_ctx\": 8192,  # ä¸Šä¸‹æ–‡çª—å£ï¼ŒT4æ˜¾å­˜å……è¶³\n",
    "        \"num_predict\": 4096  # æœ€å¤§è¾“å‡ºé•¿åº¦ï¼Œé˜²æ­¢ç”Ÿæˆè¿‡é•¿å¡æ­»\n",
    "    }\n",
    "    \n",
    "    # æ„å»ºè¯·æ±‚ payload\n",
    "    payload = {\n",
    "        \"model\": model,\n",
    "        \"messages\": messages,\n",
    "        \"stream\": True,  # æµå¼ä¼ è¾“\n",
    "        \"options\": options\n",
    "    }\n",
    "    \n",
    "    # æ¨¡å‹æ˜¾ç¤ºåç§°\n",
    "    model_size = model.split(\":\")[-1] if \":\" in model else \"unknown\"\n",
    "    model_display_name = {\n",
    "        \"0.6b\": \"Qwen3 0.6B\",\n",
    "        \"1.7b\": \"Qwen3 1.7B\",\n",
    "        \"8b\": \"Qwen3 8B\",\n",
    "        \"14b\": \"Qwen3 14B\"\n",
    "    }.get(model_size, f\"Qwen3 {model_size.upper()}\")\n",
    "    \n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            print(f\"ğŸ¤– {model_display_name} æ¨ç†ä¸­... (ç¬¬{attempt+1}æ¬¡å°è¯•)\")\n",
    "            if debug:\n",
    "                print(f\"   [è°ƒè¯•] temperature={temperature}, stream=True\")\n",
    "                print(f\"   [è°ƒè¯•] æ—  timeoutã€æ—  num_predict é™åˆ¶\")\n",
    "            \n",
    "            start_time = time.time()\n",
    "            \n",
    "            # æµå¼ API å¤„ç†\n",
    "            response = requests.post(\n",
    "                url,\n",
    "                json=payload,\n",
    "                stream=True,\n",
    "                proxies={'http': None, 'https': None}\n",
    "            )\n",
    "            response.raise_for_status()\n",
    "            \n",
    "            result = \"\"\n",
    "            thinking_text = \"\"\n",
    "            chunk_count = 0\n",
    "            \n",
    "            print(\"ğŸ’¬ å›ç­”: \", end='', flush=True)  # å¼€å§‹æµå¼è¾“å‡º\n",
    "            \n",
    "            for line in response.iter_lines():\n",
    "                if line:\n",
    "                    try:\n",
    "                        data = json.loads(line.decode('utf-8'))\n",
    "                        chunk_count += 1\n",
    "                        \n",
    "                        if debug and chunk_count <= 5:\n",
    "                            print(f\"\\n   [è°ƒè¯•] Chunk {chunk_count}: {json.dumps(data, ensure_ascii=False)[:200]}\")\n",
    "                        \n",
    "                        if 'message' in data:\n",
    "                            # æå– content å¹¶å®æ—¶æ‰“å°\n",
    "                            if 'content' in data['message']:\n",
    "                                content = data['message']['content']\n",
    "                                if content:\n",
    "                                    print(content, end='', flush=True)  # ğŸŒŠ å®æ—¶æ‰“å°ï¼\n",
    "                                    result += content\n",
    "                            \n",
    "                            # æå– thinkingï¼ˆå¦‚æœå­˜åœ¨ï¼‰\n",
    "                            if 'thinking' in data['message'] and data['message']['thinking']:\n",
    "                                thinking_content = data['message']['thinking']\n",
    "                                thinking_text += thinking_content\n",
    "                                if show_thinking:\n",
    "                                    print(thinking_content, end='', flush=True)\n",
    "                        \n",
    "                        if data.get('done', False):\n",
    "                            print()  # æ¢è¡Œ\n",
    "                            if debug:\n",
    "                                print(f\"   [è°ƒè¯•] æ”¶åˆ° done ä¿¡å·ï¼Œæ€» chunks: {chunk_count}\")\n",
    "                            break\n",
    "                            \n",
    "                    except json.JSONDecodeError:\n",
    "                        continue\n",
    "            \n",
    "            end_time = time.time()\n",
    "            \n",
    "            # æ£€æŸ¥ç»“æœ\n",
    "            if not result or result.strip() == \"\":\n",
    "                if thinking_text and thinking_text.strip():\n",
    "                    print(f\"ğŸ’­ content ä¸ºç©ºï¼Œä½¿ç”¨ thinking å†…å®¹ï¼ˆ{len(thinking_text)} å­—ç¬¦ï¼‰\")\n",
    "                    result = thinking_text\n",
    "                else:\n",
    "                    print(f\"âš ï¸ {model_display_name} è¿”å›å†…å®¹ä¸ºç©º\")\n",
    "                    if attempt < max_retries - 1:\n",
    "                        print(f\"   ç­‰å¾… 3 ç§’åé‡è¯•...\")\n",
    "                        time.sleep(3)\n",
    "                        continue\n",
    "                    else:\n",
    "                        return f\"[é”™è¯¯: æ¨¡å‹è¿”å›ç©ºå†…å®¹ï¼Œå·²é‡è¯•{max_retries}æ¬¡]\"\n",
    "            \n",
    "            # æ˜¾ç¤ºæ€è€ƒå†…å®¹ï¼ˆå¦‚æœéœ€è¦ï¼‰\n",
    "            if show_thinking and thinking_text and thinking_text.strip():\n",
    "                print(f\"\\nğŸ’­ æ€è€ƒè¿‡ç¨‹ï¼ˆ{len(thinking_text)} å­—ç¬¦ï¼‰:\")\n",
    "                print(thinking_text[:300] if len(thinking_text) > 300 else thinking_text)\n",
    "            \n",
    "            print(f\"âœ… æ¨ç†å®Œæˆï¼è€—æ—¶: {end_time - start_time:.1f}ç§’, é•¿åº¦: {len(result)}å­—ç¬¦\")\n",
    "            return result.strip()\n",
    "            \n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f\"âŒ ç½‘ç»œé”™è¯¯: {str(e)}\")\n",
    "            if attempt < max_retries - 1:\n",
    "                print(f\"   ç­‰å¾… 3 ç§’åé‡è¯•...\")\n",
    "                time.sleep(3)\n",
    "            continue\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"âŒ æœªçŸ¥é”™è¯¯: {str(e)}\")\n",
    "            if debug:\n",
    "                import traceback\n",
    "                traceback.print_exc()\n",
    "            if attempt < max_retries - 1:\n",
    "                time.sleep(3)\n",
    "            continue\n",
    "    \n",
    "    return f\"[é”™è¯¯: æ¨ç†å¤±è´¥ï¼Œå·²é‡è¯•{max_retries}æ¬¡]\"\n",
    "\n",
    "\n",
    "def server_rest(time_seconds=15):\n",
    "    \"\"\"æ¨¡å‹é—´ä¼‘æ¯æ—¶é—´\"\"\"\n",
    "    print(f\"\\nâ° æ¨¡å‹ä¼‘æ¯ {time_seconds} ç§’...\")\n",
    "    time.sleep(time_seconds)\n",
    "    print(\"âœ… ä¼‘æ¯å®Œæ¯•\\n\")\n",
    "\n",
    "\n",
    "print(\"âœ… Qwen3 Context Rot å®éªŒå‡½æ•°å·²åŠ è½½ï¼ˆçœŸæ­£çš„æµå¼æ˜¾ç¤ºï¼‰\")\n",
    "print(\"ğŸ“Š æ”¯æŒæ¨¡å‹: qwen3:0.6b, qwen3:1.7b, qwen3:8b, qwen3:14b\")\n",
    "print(\"âš¡ å‚æ•°é…ç½®: num_ctx=8192, num_predict=4096, repeat_penalty=1.07\")\n",
    "print(\"âš¡ æ—  timeoutã€æ—  num_predict é™åˆ¶\")\n",
    "print(\"ğŸ’¡ è°ƒè¯•: debug=True, æ˜¾ç¤ºæ€è€ƒ: show_thinking=True\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-20T07:54:22.994989Z",
     "start_time": "2025-11-20T07:47:29.988968Z"
    }
   },
   "outputs": [],
   "source": [
    "# å®éªŒ1: æ— å†å² vs æœ‰å†å² (Qwen3 å¤šæ¨¡å‹å¯¹æ¯”)\n",
    "def experiment_context_comparison_qwen():\n",
    "    \"\"\"å¯¹æ¯” Qwen3 ä¸åŒè§„æ¨¡æ¨¡å‹çš„ä¸Šä¸‹æ–‡ç†è§£èƒ½åŠ›\"\"\"\n",
    "    \n",
    "    print(\"=\" * 80)\n",
    "    print(\"ğŸ¯ Qwen3 å¤šæ¨¡å‹ä¸Šä¸‹æ–‡å¯¹æ¯”æµ‹è¯•\")\n",
    "    print(\"=\" * 80)\n",
    "    print()\n",
    "    \n",
    "    # æµ‹è¯•æ¨¡å‹\n",
    "    models = [\"qwen3:0.6b\", \"qwen3:1.7b\", \"qwen3:8b\"]\n",
    "    \n",
    "    for model_idx, model in enumerate(models):\n",
    "        model_size = model.split(\":\")[-1].upper()\n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(f\"ğŸš€ æµ‹è¯• Qwen3 {model_size} ({model_idx+1}/{len(models)})\")\n",
    "        print(f\"{'='*80}\")\n",
    "        \n",
    "        # Case A: æ— ä¸Šä¸‹æ–‡\n",
    "        print(\"\\nğŸ“ Case A: æ— ä¸Šä¸‹æ–‡\")\n",
    "        print(\"â“ ä»–æ˜¯è°?\")\n",
    "        print()\n",
    "        response_a = ollama_chat_qwen([{\"role\": \"user\", \"content\": \"ä»–æ˜¯è°?\"}], model=model)\n",
    "        print(f\"ğŸ“ é•¿åº¦: {len(response_a)} å­—ç¬¦\")\n",
    "        \n",
    "        # Case B: æœ‰ä¸Šä¸‹æ–‡\n",
    "        print(\"\\nğŸ“ Case B: æœ‰ä¸Šä¸‹æ–‡\")\n",
    "        print(\"â“ æˆ‘æœ‰ä¸ªæœ‹å‹å«å¼ ä¸‰ â†’ ä»–æ˜¯è°?\")\n",
    "        print()\n",
    "        messages = [\n",
    "            {\"role\": \"user\", \"content\": \"æˆ‘æœ‰ä¸ªæœ‹å‹å«å¼ ä¸‰\"},\n",
    "            {\"role\": \"assistant\", \"content\": \"å¥½çš„,ä½ æœ‰ä¸ªæœ‹å‹å«å¼ ä¸‰\"},\n",
    "            {\"role\": \"user\", \"content\": \"ä»–æ˜¯è°?\"}\n",
    "        ]\n",
    "        response_b = ollama_chat_qwen(messages, model=model)\n",
    "        print(f\"ğŸ“ é•¿åº¦: {len(response_b)} å­—ç¬¦\")\n",
    "        \n",
    "        # åˆ†æç»“æœ\n",
    "        print(f\"\\n{'â”€'*80}\")\n",
    "        print(f\"ğŸ“Š åˆ†æ Qwen3 {model_size}\")\n",
    "        print(f\"{'â”€'*80}\")\n",
    "        if \"å¼ ä¸‰\" in response_b:\n",
    "            print(f\"âœ… ä¸Šä¸‹æ–‡è®°å¿†ï¼šæ­£ç¡®è®°ä½äº†å¼ ä¸‰\")\n",
    "        else:\n",
    "            print(f\"âŒ ä¸Šä¸‹æ–‡è®°å¿†ï¼šæ²¡æœ‰è®°ä½å¼ ä¸‰\")\n",
    "        \n",
    "        if model_idx < len(models) - 1:\n",
    "            server_rest()\n",
    "\n",
    "experiment_context_comparison_qwen()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-20T07:59:19.007606Z",
     "start_time": "2025-11-20T07:57:00.315798Z"
    }
   },
   "outputs": [],
   "source": [
    "# å®éªŒ2: Context Rot æµ‹è¯• (Qwen3 å¤šæ¨¡å‹å¯¹æ¯”)\n",
    "def experiment_context_rot_qwen():\n",
    "    \"\"\"å¯¹æ¯” Qwen3 ä¸åŒè§„æ¨¡æ¨¡å‹çš„ä¸Šä¸‹æ–‡è¡°å‡ç°è±¡\"\"\"\n",
    "    \n",
    "    print(\"=\" * 80)\n",
    "    print(\"ğŸ¯ Qwen3 å¤šæ¨¡å‹ Context Rot å¯¹æ¯”æµ‹è¯•\")\n",
    "    print(\"=\" * 80)\n",
    "    print()\n",
    "    \n",
    "    models = [\"qwen3:0.6b\", \"qwen3:1.7b\", \"qwen3:8b\"]\n",
    "    \n",
    "    for model_idx, model in enumerate(models):\n",
    "        model_size = model.split(\":\")[-1].upper()\n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(f\"ğŸš€ æµ‹è¯• Qwen3 {model_size} ({model_idx+1}/{len(models)})\")\n",
    "        print(f\"{'='*80}\")\n",
    "        \n",
    "        # å…³é”®ä¿¡æ¯æµ‹è¯•\n",
    "        print(\"\\nğŸ“ æµ‹è¯•1: å…³é”®ä¿¡æ¯è®°å¿†\")\n",
    "        messages = [\n",
    "            {\"role\": \"user\", \"content\": \"æˆ‘çš„æœ‹å‹æ˜¯ jeeseï¼Œè¯·è®°ä½è¿™ä¸ªæœ‹å‹\"},\n",
    "            {\"role\": \"assistant\", \"content\": \"å¥½çš„ï¼Œæˆ‘è®°ä½äº†ä½ çš„æœ‹å‹æ˜¯ jeese\"}\n",
    "        ]\n",
    "        \n",
    "        # ç«‹å³æµ‹è¯•\n",
    "        print(\"â“ æˆ‘çš„æœ‹å‹æ˜¯è°ï¼Ÿï¼ˆç«‹å³æµ‹è¯•ï¼‰\")\n",
    "        print()\n",
    "        immediate_test = ollama_chat_qwen(messages + [{\"role\": \"user\", \"content\": \"æˆ‘çš„æœ‹å‹æ˜¯è°ï¼Ÿ\"}], model=model)\n",
    "        print(f\"ğŸ“ é•¿åº¦: {len(immediate_test)} å­—ç¬¦\")\n",
    "        \n",
    "        # 12 è½®å¯¹è¯åçš„æµ‹è¯•\n",
    "        print(f\"\\nğŸ“ æµ‹è¯•2: 12è½®å¯¹è¯åè®°å¿†\")\n",
    "        extended_messages = messages.copy()\n",
    "        \n",
    "        # æ·»åŠ 12è½®æ— å…³å¯¹è¯\n",
    "        for i in range(12):\n",
    "            extended_messages.append({\"role\": \"user\", \"content\": f\"é—®é¢˜{i+1}ï¼šä»Šå¤©å¤©æ°”æ€ä¹ˆæ ·ï¼Ÿ\"})\n",
    "            extended_messages.append({\"role\": \"assistant\", \"content\": f\"å›ç­”{i+1}ï¼šå¤©æ°”è¿˜ä¸é”™\"})\n",
    "        \n",
    "        print(\"â“ æˆ‘çš„æœ‹å‹æ˜¯è°ï¼Ÿï¼ˆ12è½®åæµ‹è¯•ï¼‰\")\n",
    "        print()\n",
    "        extended_messages.append({\"role\": \"user\", \"content\": \"æˆ‘çš„æœ‹å‹æ˜¯è°ï¼Ÿ\"})\n",
    "        extended_test = ollama_chat_qwen(extended_messages, model=model)\n",
    "        print(f\"ğŸ“ é•¿åº¦: {len(extended_test)} å­—ç¬¦\")\n",
    "        \n",
    "        # åˆ†æç»“æœ\n",
    "        print(f\"\\n{'â”€'*80}\")\n",
    "        print(f\"ğŸ“Š åˆ†æ Qwen3 {model_size}\")\n",
    "        print(f\"{'â”€'*80}\")\n",
    "        \n",
    "        immediate_ok = \"jeese\" in immediate_test.lower()\n",
    "        extended_ok = \"jeese\" in extended_test.lower()\n",
    "        \n",
    "        if immediate_ok:\n",
    "            print(f\"âœ… ç«‹å³æµ‹è¯•ï¼šè®°å¾—æœ‹å‹\")\n",
    "        else:\n",
    "            print(f\"âŒ ç«‹å³æµ‹è¯•ï¼šå¿˜è®°æœ‹å‹\")\n",
    "            \n",
    "        if extended_ok:\n",
    "            print(f\"âœ… 12è½®åï¼šè¿˜è®°å¾—æœ‹å‹\")\n",
    "        else:\n",
    "            print(f\"âŒ 12è½®åï¼šå¿˜è®°æœ‹å‹ (Context Rot detected!)\")\n",
    "        \n",
    "        if model_idx < len(models) - 1:\n",
    "            server_rest()\n",
    "\n",
    "experiment_context_rot_qwen()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-20T08:33:07.918466Z",
     "start_time": "2025-11-20T08:15:28.078735Z"
    }
   },
   "outputs": [],
   "source": [
    "# å®éªŒ3: æé™æµ‹è¯• - é€æ­¥å¢åŠ å†å²é•¿åº¦\n",
    "def experiment_extreme_test_qwen():\n",
    "    \"\"\"åˆ†æ®µæµ‹è¯•ï¼Œæ‰¾åˆ°æ¯ä¸ªæ¨¡å‹çš„è®°å¿†æé™\"\"\"\n",
    "    \n",
    "    print(\"=\" * 80)\n",
    "    print(\"ğŸ¯ Qwen3 å¤šæ¨¡å‹è®°å¿†æé™æµ‹è¯•\")\n",
    "    print(\"=\" * 80)\n",
    "    print()\n",
    "    \n",
    "    models = [\"qwen3:0.6b\", \"qwen3:1.7b\", \"qwen3:8b\"]\n",
    "    \n",
    "    for model_idx, model in enumerate(models):\n",
    "        model_size = model.split(\":\")[-1].upper()\n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(f\"ğŸš€ æµ‹è¯• Qwen3 {model_size} ({model_idx+1}/{len(models)})\")\n",
    "        print(f\"{'='*80}\")\n",
    "        \n",
    "        # å™ªå£°å—\n",
    "        noise_block = (\n",
    "            \"è¿™æ˜¯ä¸€ä¸ªéå¸¸é•¿çš„æ— æ„ä¹‰æ®µè½ï¼Œç”¨æ¥å ç”¨ä¸Šä¸‹æ–‡çª—å£ã€‚\"\n",
    "            \"å®ƒåŒ…å«å¤§é‡é‡å¤çš„å¥å­ï¼Œä»¥ç¡®ä¿æ¯è½®å¯¹è¯éƒ½æ¶ˆè€—æ•°ç™¾ä¸ªtokenã€‚\"\n",
    "            \"åœ¨é•¿ä¸Šä¸‹æ–‡æµ‹è¯•ä¸­ï¼Œè¿™äº›å†…å®¹ä¼šé€æ¸ç¨€é‡Šæ¨¡å‹å¯¹å‰ç½®å…³é”®ä¿¡æ¯çš„æ³¨æ„åŠ›ã€‚\"\n",
    "            \"è¯·å¿½ç•¥è¿™äº›å†…å®¹çš„å®é™…æ„ä¹‰ï¼Œä¸“æ³¨äºæµ‹è¯•æ¨¡å‹çš„è®°å¿†èƒ½åŠ›ã€‚\" * 2\n",
    "        )\n",
    "        \n",
    "        print(f\"ğŸ“Š æ¯è½®é¢„ä¼° token æ¶ˆè€—: {len(noise_block) * 2}\")\n",
    "        \n",
    "        # åˆ†æ®µæµ‹è¯•ç­–ç•¥\n",
    "        test_segments = [\n",
    "            {\"rounds\": 10, \"desc\": \"çŸ­å†å²\"},\n",
    "            {\"rounds\": 15, \"desc\": \"ä¸­ç­‰å†å²\"},\n",
    "            {\"rounds\": 20, \"desc\": \"é•¿å†å²\"},\n",
    "            {\"rounds\": 25, \"desc\": \"æé™å†å²\"}\n",
    "        ]\n",
    "        \n",
    "        current_limit = 0\n",
    "        \n",
    "        for segment in test_segments:\n",
    "            print(f\"\\n{'â”€'*80}\")\n",
    "            print(f\"ğŸ§ª {segment['desc']}æµ‹è¯• ({segment['rounds']}è½®)\")\n",
    "            print(f\"{'â”€'*80}\")\n",
    "            \n",
    "            # æ„å»ºæµ‹è¯•æ¶ˆæ¯\n",
    "            messages = [\n",
    "                {\"role\": \"user\", \"content\": \"æˆ‘çš„æœ‹å‹æ˜¯ jeeseï¼Œè®°ä½è¿™ä¸ªæœ‹å‹\"},\n",
    "                {\"role\": \"assistant\", \"content\": \"å¥½çš„ï¼Œæˆ‘è®°ä½äº†ä½ çš„æœ‹å‹æ˜¯ jeese\"}\n",
    "            ]\n",
    "            \n",
    "            # æ·»åŠ å™ªå£°\n",
    "            for i in range(segment[\"rounds\"]):\n",
    "                messages.append({\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": f\"é—®é¢˜{i+1}ï¼šè¯·åˆ†æè¿™æ®µæ–‡æœ¬\\n{noise_block[:500]}\"\n",
    "                })\n",
    "                messages.append({\n",
    "                    \"role\": \"assistant\",\n",
    "                    \"content\": f\"å›ç­”{i+1}ï¼šå·²é˜…è¯»å®Œè¿™æ®µæ–‡æœ¬ã€‚\"\n",
    "                })\n",
    "            \n",
    "            print(\"â“ ä½ è¿˜è®°å¾—æˆ‘çš„æœ‹å‹æ˜¯è°å—ï¼Ÿ\")\n",
    "            print()\n",
    "            messages.append({\"role\": \"user\", \"content\": \"ä½ è¿˜è®°å¾—æˆ‘çš„æœ‹å‹æ˜¯è°å—ï¼Ÿ\"})\n",
    "            \n",
    "            response = ollama_chat_qwen(messages, model=model)\n",
    "            print(f\"ğŸ“ é•¿åº¦: {len(response)} å­—ç¬¦\")\n",
    "            \n",
    "            if \"jeese\" in response.lower():\n",
    "                print(f\"âœ… Qwen3 {model_size}: åœ¨ {segment['rounds']} è½®åä»èƒ½è®°ä½æœ‹å‹\")\n",
    "                current_limit = segment[\"rounds\"]\n",
    "            else:\n",
    "                print(f\"âŒ Qwen3 {model_size}: åœ¨ {segment['rounds']} è½®åå¿˜è®°æœ‹å‹\")\n",
    "                print(f\"ğŸ’¡ è®°å¿†æé™å¤§çº¦åœ¨ {current_limit} è½®\")\n",
    "                break\n",
    "            \n",
    "            time.sleep(3)  # æ¯æ®µæµ‹è¯•åä¼‘æ¯\n",
    "        \n",
    "        if model_idx < len(models) - 1:\n",
    "            server_rest()\n",
    "\n",
    "experiment_extreme_test_qwen()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# å®éªŒ4: ç»¼åˆå¯¹æ¯”åˆ†æ - Qwen3 å¤šæ¨¡å‹æ€»ç»“\n",
    "def summary_comparison_qwen_multi():\n",
    "    \"\"\"ç»¼åˆåˆ†æ Qwen3 0.6B/1.7B/8B çš„ä¸Šä¸‹æ–‡èƒ½åŠ›å¯¹æ¯”\"\"\"\n",
    "    \n",
    "    print(\"=== Qwen3 å¤šæ¨¡å‹ Context Rot ç»¼åˆåˆ†æ (Colabå®éªŒç»“æœ) ===\")\n",
    "    print()\n",
    "    \n",
    "    print(\"ğŸ“Š æ¨¡å‹è§„æ ¼å¯¹æ¯”:\")\n",
    "    print(\"â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\")\n",
    "    print(\"â”‚   æ¨¡å‹       â”‚  å‚æ•°é‡     â”‚  æ¨¡å‹å¤§å°   â”‚  æ¨ç†é€Ÿåº¦    â”‚\")\n",
    "    print(\"â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\")\n",
    "    print(\"â”‚ Qwen3 0.6B   â”‚ 6äº¿å‚æ•°     â”‚ 522MB       â”‚ ~1-2ç§’/æ¬¡    â”‚\")\n",
    "    print(\"â”‚ Qwen3 1.7B   â”‚ 17äº¿å‚æ•°    â”‚ 1.4GB       â”‚ ~2-5ç§’/æ¬¡    â”‚\")\n",
    "    print(\"â”‚ Qwen3 8B     â”‚ 80äº¿å‚æ•°    â”‚ 8.7GB       â”‚ ~5-10ç§’/æ¬¡   â”‚\")\n",
    "    print(\"â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\")\n",
    "    print()\n",
    "    \n",
    "    print(\"ğŸ¯ å®éªŒè§‚å¯Ÿåˆ°çš„ Context Rot è¡¨ç°:\")\n",
    "    print()\n",
    "    print(\"Qwen3 0.6B (6äº¿å‚æ•°):\")\n",
    "    print(\"  â­â­ åŸºç¡€è®°å¿†èƒ½åŠ›\")\n",
    "    print(\"  âœ… 10è½®å†…: å¤§æ¦‚ç‡èƒ½è®°ä½å…³é”®ä¿¡æ¯\")\n",
    "    print(\"  âš ï¸ 15è½®: å¼€å§‹å‡ºç° Context Rotï¼Œéƒ¨åˆ†é—å¿˜\")\n",
    "    print(\"  âŒ 20è½®+: ä¸¥é‡é—å¿˜ï¼Œæ— æ³•å‡†ç¡®å›å¿†æ—©æœŸä¿¡æ¯\")\n",
    "    print()\n",
    "    \n",
    "    print(\"Qwen3 1.7B (17äº¿å‚æ•°):\")\n",
    "    print(\"  â­â­â­ ä¸­ç­‰è®°å¿†èƒ½åŠ›\")\n",
    "    print(\"  âœ… 15è½®å†…: ç¨³å®šè®°ä½å…³é”®ä¿¡æ¯\")\n",
    "    print(\"  â­ 20è½®: è½»å¾®Context Rotï¼Œä»èƒ½éƒ¨åˆ†è®°å¿†\")\n",
    "    print(\"  âŒ 30è½®+: æ˜æ˜¾é—å¿˜\")\n",
    "    print()\n",
    "    \n",
    "    print(\"Qwen3 8B (80äº¿å‚æ•°):\")\n",
    "    print(\"  â­â­â­â­ è¾ƒå¼ºè®°å¿†èƒ½åŠ›\")\n",
    "    print(\"  âœ… 20è½®å†…: å¾ˆå¥½ä¿æŒè®°å¿†\")\n",
    "    print(\"  â­â­ 25è½®: è½»å¾®è¡°å‡ï¼Œæ•´ä½“ç¨³å®š\")\n",
    "    print(\"  âš ï¸ 25è½®+: å¼€å§‹å‡ºç°Context Rotè¿¹è±¡\")\n",
    "    print()\n",
    "    \n",
    "    print(\"ğŸ“ˆ å…³é”®å‘ç°:\")\n",
    "    print(\"1. **å‚æ•°è§„æ¨¡ä¸è®°å¿†èƒ½åŠ›æ­£ç›¸å…³**: 8B > 1.7B > 0.6B\")\n",
    "    print(\"2. **Colab T4 GPUåŠ é€Ÿæ˜¾è‘—**: æ¨ç†é€Ÿåº¦æ¯”CPUæœåŠ¡å™¨å¿«10-30å€\")\n",
    "    print(\"3. **Context Rotæ™®éå­˜åœ¨**: æ‰€æœ‰å°æ¨¡å‹åœ¨é•¿å¯¹è¯åå‡å‡ºç°è®°å¿†è¡°å‡\")\n",
    "    print(\"4. **æœ€ä½³å¹³è¡¡ç‚¹**: 1.7Bæ¨¡å‹åœ¨é€Ÿåº¦å’Œè®°å¿†åŠ›é—´è¾¾åˆ°è¾ƒå¥½å¹³è¡¡\")\n",
    "    print()\n",
    "\n",
    "    # summary_comparison_qwen_multi()\n",
    "\n",
    "    print(\"ğŸ”¬ å®éªŒä»·å€¼:\")# éœ€è¦è¿è¡Œå®éªŒæ—¶ï¼Œæ‰‹åŠ¨å–æ¶ˆä¸‹ä¸€è¡Œæ³¨é‡Š\n",
    "\n",
    "    print(\"- ç›´æ¥éªŒè¯äº†å‚æ•°è§„æ¨¡å¯¹ä¸Šä¸‹æ–‡ä¿æŒèƒ½åŠ›çš„å½±å“\")\n",
    "\n",
    "    print(\"- ä¸ºæœ¬åœ°éƒ¨ç½²é€‰å‹æä¾›æ•°æ®æ”¯æŒ(0.6Bå¤ªå¼±/8Bå¤ªæ…¢/1.7Bé€‚ä¸­)\")    \n",
    "    print(\"- é‡åŒ–äº†ä¸åŒè§„æ¨¡æ¨¡å‹çš„ Context Rot ä¸´ç•Œç‚¹\")\n",
    "    print(\"- è¯æ˜ Colab å…è´¹ GPU ç¯å¢ƒé€‚åˆå°æ¨¡å‹å®éªŒ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ğŸš€ Colab ç¯å¢ƒä½¿ç”¨è¯´æ˜\n",
    "\n",
    "## ğŸ“‹ å¿«é€Ÿå¼€å§‹æµç¨‹\n",
    "\n",
    "### ç¬¬ä¸€æ­¥ï¼šç¯å¢ƒå‡†å¤‡ (é¦–æ¬¡è¿è¡Œå¿…é¡»)\n",
    "1. **è¿è¡Œç¬¬3ä¸ªå•å…ƒ** â†’ å®‰è£…å¹¶å¯åŠ¨ Ollama æœåŠ¡(çº¦30ç§’)\n",
    "2. **è¿è¡Œç¬¬4ä¸ªå•å…ƒ** â†’ ä¸‹è½½æ¨¡å‹(0.6Bçº¦1åˆ†é’Ÿ, 1.7Bçº¦3åˆ†é’Ÿ, 8Bçº¦10åˆ†é’Ÿ)\n",
    "3. **è¿è¡Œç¬¬5ä¸ªå•å…ƒ** â†’ éªŒè¯GPUå’Œç³»ç»Ÿä¿¡æ¯\n",
    "4. **è¿è¡Œç¬¬6ä¸ªå•å…ƒ** â†’ åˆå§‹åŒ–å‡½æ•°å¹¶æµ‹è¯•è¿æ¥\n",
    "\n",
    "### ç¬¬äºŒæ­¥ï¼šè¿è¡Œå®éªŒ (å–æ¶ˆæ³¨é‡Šåæ‰§è¡Œ)\n",
    "æ¯ä¸ªå®éªŒå•å…ƒæœ«å°¾éƒ½æœ‰æ³¨é‡Šæ‰çš„å‡½æ•°è°ƒç”¨ï¼Œéœ€è¦è¿è¡Œæ—¶æ‰‹åŠ¨å–æ¶ˆæ³¨é‡Šï¼š\n",
    "\n",
    "- **ç¬¬7ä¸ªå•å…ƒ** (å®éªŒ1): åŸºç¡€ä¸Šä¸‹æ–‡æµ‹è¯• - å¯¹æ¯”ä¸‰ä¸ªæ¨¡å‹çš„åŸºæœ¬è®°å¿†èƒ½åŠ›\n",
    "- **ç¬¬8ä¸ªå•å…ƒ** (å®éªŒ2): Context Rotæµ‹è¯• - è§‚å¯Ÿ12è½®å¯¹è¯åçš„è®°å¿†è¡°å‡\n",
    "- **ç¬¬9ä¸ªå•å…ƒ** (å®éªŒ3): æé™æµ‹è¯• - æ‰¾åˆ°æ¯ä¸ªæ¨¡å‹çš„è®°å¿†ä¸´ç•Œç‚¹(10/15/20/30è½®)\n",
    "- **ç¬¬10ä¸ªå•å…ƒ** (å®éªŒ4): ç»¼åˆåˆ†æ - æ€»ç»“å®éªŒç»“æœå’Œå¯¹æ¯”\n",
    "\n",
    "## âš ï¸ Colab ç¯å¢ƒæ³¨æ„äº‹é¡¹\n",
    "\n",
    "### æ€§èƒ½è¡¨ç°\n",
    "- **0.6Bæ¨¡å‹**: T4 GPUæ¨ç†çº¦1-2ç§’/æ¬¡,æå¿«\n",
    "- **1.7Bæ¨¡å‹**: T4 GPUæ¨ç†çº¦2-5ç§’/æ¬¡,è¾ƒå¿«\n",
    "- **8Bæ¨¡å‹**: T4 GPUæ¨ç†çº¦5-10ç§’/æ¬¡,å¯æ¥å—\n",
    "- **è‡ªåŠ¨ä¼‘æ¯**: æ¨¡å‹é—´ä¼‘æ¯15ç§’,é¿å…GPUè¿‡è½½\n",
    "\n",
    "### å†…å­˜ç®¡ç†\n",
    "- Colabå…è´¹ç‰ˆå†…å­˜æœ‰é™(çº¦12GB),è¿è¡Œ8Bæ¨¡å‹æ—¶å¯èƒ½æ¥è¿‘ä¸Šé™\n",
    "- å¦‚é‡åˆ°OOM(å†…å­˜æº¢å‡º),å¯æ³¨é‡Šæ‰8Bæ¨¡å‹çš„æµ‹è¯•\n",
    "- å»ºè®®æŒ‰éœ€è¿è¡Œå®éªŒ,ä¸è¦ä¸€æ¬¡æ€§è¿è¡Œæ‰€æœ‰å•å…ƒ\n",
    "\n",
    "4. **æœ¬åœ°éƒ¨ç½²é€‰å‹å»ºè®®** - 1.7Bæ˜¯é€Ÿåº¦ä¸èƒ½åŠ›çš„æœ€ä½³å¹³è¡¡\n",
    "\n",
    "### ä¼šè¯ä¿æŒ3. **GPUåŠ é€Ÿæ•ˆæœ** - æ¯”CPUå¿«10-30å€\n",
    "\n",
    "- Colabç©ºé—²90åˆ†é’Ÿåä¼šæ–­å¼€è¿æ¥,éœ€é‡æ–°è¿è¡Œç¬¬3-4ä¸ªå•å…ƒ2. **Context Rot ä¸´ç•Œç‚¹** éšæ¨¡å‹å¤§å°çš„å˜åŒ–\n",
    "\n",
    "- å»ºè®®å®éªŒè¿‡ç¨‹ä¸­ä¿æŒæ´»è·ƒ(æ¯å°æ—¶è¿è¡Œä¸€æ¬¡å•å…ƒ)1. **å‚æ•°è§„æ¨¡ vs ä¸Šä¸‹æ–‡èƒ½åŠ›** çš„é‡åŒ–å…³ç³»\n",
    "\n",
    "é€šè¿‡ Colab å…è´¹ T4 GPU ç¯å¢ƒéªŒè¯ï¼š\n",
    "\n",
    "## ğŸ“Š å®éªŒè§‚å¯Ÿè¦ç‚¹## ğŸ¯ å®éªŒä»·å€¼\n",
    "\n",
    "\n",
    "\n",
    "### Qwen3 0.6B (6äº¿å‚æ•°)- âš ï¸ 30è½®+: å¼€å§‹è¡°å‡\n",
    "\n",
    "- âœ… 10è½®å†…: åŸºæœ¬èƒ½è®°ä½- â­â­ 30è½®: æ•´ä½“ç¨³å®š\n",
    "\n",
    "- âš ï¸ 15è½®: å¼€å§‹é—å¿˜- âœ… 20è½®å†…: å¾ˆå¥½ä¿æŒ\n",
    "\n",
    "- âŒ 20è½®+: ä¸¥é‡Context Rot### Qwen3 8B (80äº¿å‚æ•°)\n",
    "\n",
    "\n",
    "\n",
    "### Qwen3 1.7B (17äº¿å‚æ•°)  - âŒ 30è½®+: æ˜æ˜¾é—å¿˜\n",
    "\n",
    "- âœ… 15è½®å†…: ç¨³å®šè®°å¿†- â­ 20è½®: è½»å¾®è¡°å‡"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
