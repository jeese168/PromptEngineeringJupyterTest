{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7ca4b58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Qwen3 æ™ºèƒ½å¯¹æ¯”åŸºå‡†æµ‹è¯•\n",
    "\n",
    "è¿™ä¸ªç¬”è®°æœ¬ä¸“é—¨ç”¨äºå¯¹æ¯”ä¸åŒè§„æ¨¡çš„ Qwen3 æ¨¡å‹ï¼ˆ8B vs 14B vs 32Bï¼‰çš„æ™ºèƒ½è¡¨ç°å·®å¼‚ã€‚\n",
    "\n",
    "## ğŸ¯ æµ‹è¯•ç›®æ ‡\n",
    "é€šè¿‡ç²¾å¿ƒè®¾è®¡çš„ä¸­ç­‰éš¾åº¦é—®é¢˜ï¼Œé‡åŒ–å¯¹æ¯”ä¸åŒå‚æ•°è§„æ¨¡æ¨¡å‹çš„æ™ºèƒ½å·®å¼‚ï¼ŒåŒ…æ‹¬ï¼š\n",
    "- é€»è¾‘æ¨ç†èƒ½åŠ›\n",
    "- æ•°å­¦è®¡ç®—èƒ½åŠ›  \n",
    "- çŸ¥è¯†ç†è§£æ·±åº¦\n",
    "- åˆ›æ„ç”Ÿæˆèƒ½åŠ›\n",
    "- ä»£ç å®ç°è´¨é‡\n",
    "- ä¼¦ç†æ€è€ƒæ·±åº¦\n",
    "- æŠ½è±¡æ€ç»´èƒ½åŠ›\n",
    "- å¤æ‚æ¨ç†èƒ½åŠ›\n",
    "\n",
    "## ğŸ“‹ å¿«é€Ÿå¼€å§‹\n",
    "\n",
    "### ç¬¬ä¸€æ­¥ï¼šç¯å¢ƒå‡†å¤‡\n",
    "è¿è¡Œä¸‹é¢çš„å•å…ƒæ ¼ï¼Œå®‰è£…å¹¶å¯åŠ¨ Ollama æœåŠ¡ã€‚\n",
    "\n",
    "### ç¬¬äºŒæ­¥ï¼šä¸‹è½½æ¨¡å‹\n",
    "è¿è¡Œæ¨¡å‹ä¸‹è½½å•å…ƒæ ¼ï¼Œè·å–æ‰€éœ€çš„ Qwen3 æ¨¡å‹ã€‚\n",
    "\n",
    "### ç¬¬ä¸‰æ­¥ï¼šè¿è¡Œæµ‹è¯•\n",
    "æ‰§è¡Œå®éªŒå‡½æ•°ï¼Œå¼€å§‹æ™ºèƒ½å¯¹æ¯”æµ‹è¯•ã€‚\n",
    "\n",
    "## âš ï¸ æ³¨æ„äº‹é¡¹\n",
    "- æ”¯æŒæ¨¡å‹ï¼šqwen3:8b, qwen3:14b, qwen3:32b\n",
    "- å‡½æ•°ä¼šè‡ªåŠ¨è¯†åˆ«æ¨¡å‹è§„æ¨¡å¹¶è°ƒæ•´å‚æ•°\n",
    "- 32Bæ¨¡å‹éœ€è¦æ›´å¤šå†…å­˜å’Œæ¨ç†æ—¶é—´\n",
    "- æµ‹è¯•åŒ…å«8ä¸ªç»´åº¦çš„ç»¼åˆè¯„ä¼°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da726cc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === 1. åœ¨ Colab å®‰è£…å¹¶å¯åŠ¨ Ollama ===\n",
    "import subprocess\n",
    "import time\n",
    "import os\n",
    "\n",
    "print(\"â¬‡ï¸ æ­£åœ¨å®‰è£… Ollama...\")\n",
    "install_cmd = \"curl -fsSL https://ollama.com/install.sh | sh\"\n",
    "install_result = subprocess.run(install_cmd, shell=True, check=False)\n",
    "if install_result.returncode == 0:\n",
    "    print(\"âœ… Ollama å®‰è£…å®Œæˆ\")\n",
    "else:\n",
    "    print(\"âš ï¸ Ollama å®‰è£…é‡åˆ°é—®é¢˜ï¼Œç»§ç»­å°è¯•...\")\n",
    "    # å°è¯•å¤‡ç”¨å®‰è£…æ–¹å¼\n",
    "    try:\n",
    "        subprocess.run([\"apt\", \"update\"], check=False)\n",
    "        subprocess.run([\"apt\", \"install\", \"-y\", \"curl\"], check=False)\n",
    "        install_result = subprocess.run(install_cmd, shell=True, check=False)\n",
    "        if install_result.returncode == 0:\n",
    "            print(\"âœ… Ollama å®‰è£…å®Œæˆï¼ˆé‡è¯•ï¼‰\")\n",
    "        else:\n",
    "            print(\"âš ï¸ Ollama å¯èƒ½å·²å®‰è£…æˆ–å®‰è£…å¤±è´¥ï¼Œç»§ç»­æ‰§è¡Œ...\")\n",
    "    except Exception as e:\n",
    "        print(f\"âš ï¸ å¤‡ç”¨å®‰è£…æ–¹å¼å¤±è´¥: {str(e)}ï¼Œç»§ç»­æ‰§è¡Œ...\")\n",
    "\n",
    "print(\"ğŸš€ æ­£åœ¨åå°å¯åŠ¨ Ollama æœåŠ¡...\")\n",
    "# å°†è¿›ç¨‹å¥æŸ„æš´éœ²åœ¨å…¨å±€ä½œç”¨åŸŸï¼Œæ–¹ä¾¿åç»­æ‰‹åŠ¨åœæ­¢\n",
    "ollama_server = subprocess.Popen([\"ollama\", \"serve\"], stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
    "time.sleep(5)\n",
    "print(\"âœ… Ollama æœåŠ¡å·²åœ¨ localhost:11434 è¿è¡Œï¼Œå˜é‡ ollama_server å¯ç”¨äºæ‰‹åŠ¨ç»ˆæ­¢\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac8a79ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "â¬‡ï¸ æ­£åœ¨ä¸‹è½½ qwen3:8b...\n",
      "â¬‡ï¸ æ­£åœ¨ä¸‹è½½ qwen3:14b...\n",
      "âœ… æ¨¡å‹ä¸‹è½½å®Œæˆï¼Œå¯ä»¥è¿è¡Œåç»­å®éªŒ\n"
     ]
    }
   ],
   "source": [
    "# === 2. ä¸‹è½½æ‰€éœ€æ¨¡å‹ï¼ˆå¯æ ¹æ®éœ€è¦å¢å‡ï¼‰ ===\n",
    "import subprocess\n",
    "\n",
    "\n",
    "print(\"â¬‡ï¸ æ­£åœ¨ä¸‹è½½ qwen3:8b...\")\n",
    "pull_qwen_8b = subprocess.run([\"ollama\", \"pull\", \"qwen3:8b\"], check=False)\n",
    "if pull_qwen_8b.returncode != 0:\n",
    "    raise RuntimeError(\"âŒ qwen3:8b ä¸‹è½½å¤±è´¥\")\n",
    "\n",
    "print(\"â¬‡ï¸ æ­£åœ¨ä¸‹è½½ qwen3:14b...\")\n",
    "pull_qwen_14b = subprocess.run([\"ollama\", \"pull\", \"qwen3:14b\"], check=False)\n",
    "if pull_qwen_14b.returncode != 0:\n",
    "    raise RuntimeError(\"âŒ qwen3:14b ä¸‹è½½å¤±è´¥\")\n",
    "\n",
    "\n",
    "#TODO ç”¨l4çš„æ—¶å€™è§£é™¤æ³¨é‡Š\n",
    "# print(\"â¬‡ï¸ æ­£åœ¨ä¸‹è½½ qwen3:32b...\")\n",
    "# pull_qwen_32b = subprocess.run([\"ollama\", \"pull\", \"qwen3:32b\"], check=False)\n",
    "# if pull_qwen_32b.returncode != 0:\n",
    "#     raise RuntimeError(\"âŒ qwen3:32b ä¸‹è½½å¤±è´¥\")\n",
    "\n",
    "print(\"âœ… æ¨¡å‹ä¸‹è½½å®Œæˆï¼Œå¯ä»¥è¿è¡Œåç»­å®éªŒ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "kiaktri2aw",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === 3. å¯¼å…¥å¿…è¦çš„åº“å’Œé…ç½® ===\n",
    "import requests\n",
    "import json\n",
    "import time\n",
    "\n",
    "# é…ç½® Ollama API åœ°å€\n",
    "OLLAMA_URL = \"http://localhost:11434\"\n",
    "\n",
    "print(\"âœ… ä¾èµ–åº“å¯¼å…¥å®Œæˆ\")\n",
    "print(f\"ğŸ”§ Ollama API åœ°å€: {OLLAMA_URL}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b02a501",
   "metadata": {},
   "outputs": [],
   "source": [
    "# å®šä¹‰æ ¸å¿ƒå‡½æ•°ï¼šOllama API è°ƒç”¨å‡½æ•°ï¼ˆçœŸæ­£çš„æµå¼æ˜¾ç¤º - æ— é™çº§é€»è¾‘ï¼‰\n",
    "\n",
    "def ollama_chat_qwen(messages, temperature=0.7, max_retries=3, model=\"qwen3:8b\", \n",
    "                     debug=False, enable_thinking=False, show_thinking=False):\n",
    "    \"\"\"\n",
    "    è°ƒç”¨ Qwen3 æ¨¡å‹è¿›è¡Œæ¨ç†\n",
    "    \n",
    "    å‚æ•°:\n",
    "        messages: å¯¹è¯æ¶ˆæ¯åˆ—è¡¨\n",
    "        temperature: æ¸©åº¦å‚æ•° (0.0-1.0)\n",
    "        max_retries: æœ€å¤§é‡è¯•æ¬¡æ•°\n",
    "        model: æ¨¡å‹åç§°\n",
    "        debug: æ˜¯å¦æ˜¾ç¤ºè°ƒè¯•ä¿¡æ¯\n",
    "        enable_thinking: æ˜¯å¦å¯ç”¨æ€è€ƒæ¨¡å¼ï¼ˆé»˜è®¤Falseï¼‰\n",
    "        show_thinking: æ˜¯å¦æ˜¾ç¤ºæ€è€ƒå†…å®¹ï¼ˆä»…å½“enable_thinking=Trueæ—¶æœ‰æ•ˆï¼‰\n",
    "    \n",
    "    æ³¨æ„: \n",
    "        - æœ¬åœ°æ¨ç†ï¼Œæ— timeouté™åˆ¶\n",
    "        - num_ctx=8192, num_predict=4096 (å¹³è¡¡æ˜¾å­˜ä¸æ€§èƒ½)\n",
    "        - çœŸæ­£çš„æµå¼æ˜¾ç¤ºï¼Œå®æ—¶æ‰“å°æ¯ä¸ªtoken\n",
    "        - æ— é™çº§é€»è¾‘ï¼Œæµå¼å¤±è´¥å°±å¤±è´¥\n",
    "    \"\"\"\n",
    "    url = f\"{OLLAMA_URL}/api/chat\"\n",
    "    \n",
    "    # ä¼˜åŒ–åçš„å‚æ•°é…ç½®\n",
    "    options = {\n",
    "        \"temperature\": temperature,\n",
    "        \"top_p\": 0.9,\n",
    "        \"repeat_penalty\": 1.07,  # é™ä½æƒ©ç½šç‡ï¼Œé¿å…è¿‡åº¦å›é¿é‡å¤\n",
    "        \"num_ctx\": 8192,  # ä¸Šä¸‹æ–‡çª—å£ï¼ŒT4/L4æ˜¾å­˜å……è¶³\n",
    "        \"num_predict\": 4096  # æœ€å¤§è¾“å‡ºé•¿åº¦ï¼Œé˜²æ­¢ç”Ÿæˆè¿‡é•¿å¡æ­»\n",
    "    }\n",
    "    \n",
    "    # æ„å»ºè¯·æ±‚payload\n",
    "    payload = {\n",
    "        \"model\": model,\n",
    "        \"messages\": messages,\n",
    "        \"stream\": True,  # å§‹ç»ˆä½¿ç”¨æµå¼\n",
    "        \"think\": enable_thinking,\n",
    "        \"options\": options\n",
    "    }\n",
    "    \n",
    "    # æ¨¡å‹æ˜¾ç¤ºåç§°\n",
    "    model_size = model.split(\":\")[-1] if \":\" in model else \"8b\"\n",
    "    model_display_name = {\n",
    "        \"32b\": \"Qwen3 32B\",\n",
    "        \"14b\": \"Qwen3 14B\", \n",
    "        \"8b\": \"Qwen3 8B\",\n",
    "        \"1.7b\": \"Qwen3 1.7B\"\n",
    "    }.get(model_size, f\"Qwen3 {model_size}\")\n",
    "    \n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            print(f\"ğŸ¤– {model_display_name} æ¨ç†ä¸­... (ç¬¬{attempt+1}æ¬¡å°è¯•)\")\n",
    "            if debug:\n",
    "                print(f\"   [è°ƒè¯•] æœ¬åœ°æ¨ç†ï¼šnum_ctx=8192, num_predict=4096, repeat_penalty=1.07\")\n",
    "                print(f\"   [è°ƒè¯•] æœ¬åœ°æ¨ç†ï¼šæ— timeoutã€æ— num_ctxã€æ— num_predicté™åˆ¶\")\n",
    "            \n",
    "            start_time = time.time()\n",
    "            \n",
    "            # æµå¼APIå¤„ç†\n",
    "            response = requests.post(url, json=payload, stream=True)\n",
    "            response.raise_for_status()\n",
    "            \n",
    "            result = \"\"\n",
    "            thinking_text = \"\"\n",
    "            \n",
    "            print(\"ğŸ’¬ å›ç­”: \", end='', flush=True)  # å¼€å§‹æµå¼è¾“å‡º\n",
    "            \n",
    "            for line in response.iter_lines():\n",
    "                if line:\n",
    "                    try:\n",
    "                        data = json.loads(line.decode('utf-8'))\n",
    "                        \n",
    "                        if 'message' in data:\n",
    "                            # æå– content å¹¶å®æ—¶æ‰“å°\n",
    "                            if 'content' in data['message']:\n",
    "                                content = data['message']['content']\n",
    "                                if content:\n",
    "                                    print(content, end='', flush=True)  # ğŸŒŠ å®æ—¶æ‰“å°ï¼\n",
    "                                    result += content\n",
    "                            \n",
    "                            # æå– thinkingï¼ˆå¦‚æœå­˜åœ¨ï¼‰\n",
    "                            if 'thinking' in data['message'] and data['message']['thinking']:\n",
    "                                thinking_content = data['message']['thinking']\n",
    "                                thinking_text += thinking_content\n",
    "                                if show_thinking:\n",
    "                                    print(thinking_content, end='', flush=True)\n",
    "                        \n",
    "                        if data.get('done', False):\n",
    "                            print()  # æ¢è¡Œ\n",
    "                            break\n",
    "                            \n",
    "                    except json.JSONDecodeError:\n",
    "                        continue\n",
    "            \n",
    "            end_time = time.time()\n",
    "            \n",
    "            # æ£€æŸ¥ç»“æœ\n",
    "            if not result or result.strip() == \"\":\n",
    "                if thinking_text and thinking_text.strip():\n",
    "                    print(f\"ğŸ’­ contentä¸ºç©ºï¼Œä½¿ç”¨thinkingå†…å®¹ï¼ˆ{len(thinking_text)}å­—ç¬¦ï¼‰\")\n",
    "                    result = thinking_text\n",
    "                else:\n",
    "                    print(f\"âš ï¸ {model_display_name} è¿”å›å†…å®¹ä¸ºç©º\")\n",
    "                    if attempt < max_retries - 1:\n",
    "                        print(f\"   ç­‰å¾…3ç§’åé‡è¯•...\")\n",
    "                        time.sleep(3)\n",
    "                        continue\n",
    "                    else:\n",
    "                        return f\"[é”™è¯¯: æ¨¡å‹è¿”å›ç©ºå†…å®¹ï¼Œå·²é‡è¯•{max_retries}æ¬¡]\"\n",
    "            \n",
    "            # æ˜¾ç¤ºæ€è€ƒå†…å®¹ï¼ˆå¦‚æœéœ€è¦ï¼‰\n",
    "            if show_thinking and thinking_text and thinking_text.strip():\n",
    "                print(f\"\\nğŸ’­ æ€è€ƒè¿‡ç¨‹ï¼ˆ{len(thinking_text)}å­—ç¬¦ï¼‰:\")\n",
    "                print(f\"{thinking_text[:300]}...\" if len(thinking_text) > 300 else thinking_text)\n",
    "            \n",
    "            print(f\"âœ… æ¨ç†å®Œæˆï¼è€—æ—¶: {end_time - start_time:.1f}ç§’, é•¿åº¦: {len(result)}å­—ç¬¦\")\n",
    "            return result.strip()\n",
    "            \n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f\"âŒ ç½‘ç»œé”™è¯¯: {str(e)}\")\n",
    "            if attempt < max_retries - 1:\n",
    "                print(f\"   ç­‰å¾…3ç§’åé‡è¯•...\")\n",
    "                time.sleep(3)\n",
    "            continue\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"âŒ æœªçŸ¥é”™è¯¯: {str(e)}\")\n",
    "            if debug:\n",
    "                import traceback\n",
    "                traceback.print_exc()\n",
    "            if attempt < max_retries - 1:\n",
    "                time.sleep(3)\n",
    "            continue\n",
    "    \n",
    "    return f\"[é”™è¯¯: æ¨ç†å¤±è´¥ï¼Œå·²é‡è¯•{max_retries}æ¬¡]\"\n",
    "\n",
    "print(\"âœ… Qwen3 æ™ºèƒ½å¯¹æ¯”æµ‹è¯•å‡½æ•°å·²åŠ è½½ï¼ˆçº¯æµå¼ï¼Œæ— é™çº§ï¼‰\")\n",
    "print(\"ğŸ“Š æ”¯æŒæ¨¡å‹: qwen3:8b, qwen3:14b\")\n",
    "#TODO ç”¨l4çš„æ—¶å€™è§£é™¤æ³¨é‡Š\n",
    "# print(\"ğŸ“Š L4 GPU: qwen3:32b ä¹Ÿå¯ç”¨\")\n",
    "print(\"âš¡ å‚æ•°é…ç½®: num_ctx=8192, num_predict=4096, repeat_penalty=1.07\")\n",
    "print(\"âš¡ æ— timeoutã€æ— num_ctxã€æ— num_predicté™åˆ¶\")\n",
    "print(\"ğŸŒŠ æµå¼æ˜¾ç¤ºï¼šå®æ—¶æ‰“å°æ¯ä¸ªtokenï¼ŒåƒChatGPTä¸€æ ·\")\n",
    "print(\"ğŸ’¡ è°ƒè¯•: debug=True, æ˜¾ç¤ºæ€è€ƒ: show_thinking=True\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fa59ad4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== Qwen3 æ™ºèƒ½å¯¹æ¯”æµ‹è¯•å®éªŒï¼ˆæµå¼æ˜¾ç¤º + æ— æˆªæ–­ï¼‰==========\n",
    "\n",
    "def experiment_8b_vs_14b_intelligence(debug=False, models=None, \n",
    "                                      enable_thinking=False, show_thinking=False):\n",
    "    \"\"\"\n",
    "    å¯¹æ¯”ä¸åŒè§„æ¨¡ Qwen3 æ¨¡å‹çš„æ™ºèƒ½ç¨‹åº¦å·®å¼‚\n",
    "    \n",
    "    âš¡ ä¼˜åŒ–: æŒ‰æ¨¡å‹å¾ªç¯è€ŒéæŒ‰é—®é¢˜å¾ªç¯ï¼Œå‡å°‘æ¨¡å‹åˆ‡æ¢æ¬¡æ•°ï¼Œæå‡æ˜¾å­˜æ•ˆç‡\n",
    "    ğŸ”§ ä¿®å¤: ç¦ç”¨thinkingæ¨¡å¼ï¼Œé¿å…thinkingå ç”¨æ‰€æœ‰tokenå¯¼è‡´contentä¸ºç©º\n",
    "    ğŸŒŠ æµå¼æ˜¾ç¤º: å®æ—¶æ‰“å°æ¯ä¸ªtokenï¼Œå®Œæ•´æ˜¾ç¤ºæ‰€æœ‰å†…å®¹\n",
    "    \n",
    "    å‚æ•°:\n",
    "        debug: æ˜¯å¦å¼€å¯è°ƒè¯•æ¨¡å¼\n",
    "        models: è¦æµ‹è¯•çš„æ¨¡å‹åˆ—è¡¨ï¼Œé»˜è®¤ä¸º [\"qwen3:8b\", \"qwen3:14b\"]\n",
    "                #TODO ç”¨l4çš„æ—¶å€™è§£é™¤æ³¨é‡Š: å¯ä»¥æ·»åŠ  \"qwen3:32b\"\n",
    "        enable_thinking: æ˜¯å¦å¯ç”¨Qwen3çš„æ€è€ƒæ¨¡å¼ï¼ˆé»˜è®¤Falseï¼Œæ¨èå…³é—­ï¼‰\n",
    "        show_thinking: æ˜¯å¦æ˜¾ç¤ºæ€è€ƒè¿‡ç¨‹ï¼ˆä»…å½“enable_thinking=Trueæ—¶æœ‰æ•ˆï¼‰\n",
    "    \"\"\"\n",
    "    \n",
    "    if models is None:\n",
    "        models = [\"qwen3:8b\", \"qwen3:14b\"]\n",
    "        #TODO ç”¨l4çš„æ—¶å€™è§£é™¤æ³¨é‡Š\n",
    "        # models = [\"qwen3:8b\", \"qwen3:14b\", \"qwen3:32b\"]\n",
    "    \n",
    "    print(\"=\" * 80)\n",
    "    print(\"ğŸ¯ Qwen3 æ™ºèƒ½å¯¹æ¯”æµ‹è¯•ï¼ˆæµå¼æ˜¾ç¤º + æ— æˆªæ–­ï¼‰\")\n",
    "    print(\"=\" * 80)\n",
    "    print(f\"ğŸ“Š æµ‹è¯•æ¨¡å‹: {', '.join(models)}\")\n",
    "    print(f\"ğŸ”§ è°ƒè¯•æ¨¡å¼: {'å¼€å¯' if debug else 'å…³é—­'}\")\n",
    "    print(f\"ğŸ’­ æ€è€ƒæ¨¡å¼: {'å¼€å¯' if enable_thinking else 'å…³é—­ï¼ˆæ¨èï¼‰'}\")\n",
    "    print(f\"âš¡ ä¼˜åŒ–ç­–ç•¥: æ¯ä¸ªæ¨¡å‹å®Œæ•´è¿è¡Œæ‰€æœ‰é—®é¢˜åå†åˆ‡æ¢ï¼ˆå‡å°‘æ˜¾å­˜æ¬è¿ï¼‰\")\n",
    "    print(f\"ğŸŒŠ æ˜¾ç¤ºæ¨¡å¼: å®æ—¶æµå¼æ˜¾ç¤ºï¼Œå®Œæ•´å†…å®¹æ— æˆªæ–­\")\n",
    "    print(\"=\" * 80)\n",
    "    print()\n",
    "    \n",
    "    # ç²¾å¿ƒè®¾è®¡çš„æµ‹è¯•é—®é¢˜é›†\n",
    "    test_questions = [\n",
    "        {\n",
    "            \"id\": 1,\n",
    "            \"category\": \"é€»è¾‘æ¨ç†\",\n",
    "            \"question\": \"å¦‚æœæ‰€æœ‰çš„çŒ«éƒ½ä¼šçˆ¬æ ‘ï¼Œè€Œå°èŠ±æ˜¯ä¸€åªçŒ«ï¼Œé‚£ä¹ˆå°èŠ±ä¼šçˆ¬æ ‘å—ï¼Ÿè¯·è§£é‡Šä½ çš„æ¨ç†è¿‡ç¨‹ã€‚\",\n",
    "            \"metrics\": [\"logic\"]\n",
    "        },\n",
    "        {\n",
    "            \"id\": 2,\n",
    "            \"category\": \"æ•°å­¦æ¨ç†\",\n",
    "            \"question\": \"ä¸€ä¸ªæ°´æ± æœ‰ä¸€ä¸ªè¿›æ°´ç®¡å’Œä¸€ä¸ªå‡ºæ°´ç®¡ã€‚è¿›æ°´ç®¡å•ç‹¬å¼€éœ€è¦6å°æ—¶æ³¨æ»¡æ°´æ± ï¼Œå‡ºæ°´ç®¡å•ç‹¬å¼€éœ€è¦8å°æ—¶æ’ç©ºæ»¡æ± ã€‚å¦‚æœä¸¤ä¸ªæ°´ç®¡åŒæ—¶æ‰“å¼€ï¼Œéœ€è¦å¤šé•¿æ—¶é—´æ‰èƒ½æ³¨æ»¡ç©ºæ± ï¼Ÿ\",\n",
    "            \"metrics\": [\"math\", \"logic\"]\n",
    "        },\n",
    "        {\n",
    "            \"id\": 3,\n",
    "            \"category\": \"çŸ¥è¯†ç†è§£\",\n",
    "            \"question\": \"è¯·è§£é‡Šé‡å­çº ç¼ ç°è±¡ï¼Œå¹¶ä¸¾ä¸€ä¸ªå®é™…åº”ç”¨çš„ä¾‹å­ã€‚\",\n",
    "            \"metrics\": [\"knowledge\", \"depth\"]\n",
    "        },\n",
    "        {\n",
    "            \"id\": 4,\n",
    "            \"category\": \"åˆ›æ„å†™ä½œ\",\n",
    "            \"question\": \"è¯·ä»¥'æ—¶é—´æ—…è¡Œè€…æ„å¤–æ”¹å˜äº†å†å²'ä¸ºé¢˜ï¼Œå†™ä¸€æ®µ100å­—å·¦å³çš„ç§‘å¹»çŸ­æ–‡ã€‚\",\n",
    "            \"metrics\": [\"creativity\"]\n",
    "        },\n",
    "        {\n",
    "            \"id\": 5,\n",
    "            \"category\": \"ä»£ç èƒ½åŠ›\",\n",
    "            \"question\": \"è¯·ç”¨Pythonå†™ä¸€ä¸ªå‡½æ•°ï¼Œæ‰¾å‡ºåˆ—è¡¨ä¸­å‡ºç°æ¬¡æ•°æœ€å¤šçš„å…ƒç´ ï¼Œå¹¶è¿”å›è¯¥å…ƒç´ åŠå…¶å‡ºç°æ¬¡æ•°ã€‚\",\n",
    "            \"metrics\": [\"code\"]\n",
    "        },\n",
    "        {\n",
    "            \"id\": 6,\n",
    "            \"category\": \"ä¼¦ç†æ€è€ƒ\",\n",
    "            \"question\": \"å¦‚æœAIæŠ€æœ¯èƒ½å¤Ÿå®Œç¾é¢„æµ‹äººç±»çš„çŠ¯ç½ªè¡Œä¸ºï¼Œæˆ‘ä»¬åº”è¯¥åœ¨çŠ¯ç½ªå‘ç”Ÿå‰å°±é€®æ•å«Œç–‘äººå—ï¼Ÿä¸ºä»€ä¹ˆï¼Ÿ\",\n",
    "            \"metrics\": [\"ethics\", \"depth\"]\n",
    "        },\n",
    "        {\n",
    "            \"id\": 7,\n",
    "            \"category\": \"æŠ½è±¡æ€ç»´\",\n",
    "            \"question\": \"è¯·è§£é‡Š'è‡ªç”±æ„å¿—'è¿™ä¸ªæ¦‚å¿µï¼Œå¹¶è®¨è®ºå®ƒæ˜¯å¦å¯èƒ½å­˜åœ¨ã€‚\",\n",
    "            \"metrics\": [\"philosophy\", \"depth\"]\n",
    "        },\n",
    "        {\n",
    "            \"id\": 8,\n",
    "            \"category\": \"å¤æ‚æ¨ç†\",\n",
    "            \"question\": \"æœ‰ä¸‰ä¸ªäººç«™åœ¨æ¡¥ä¸Šï¼šä¸€ä¸ªæ€»æ˜¯è¯´çœŸè¯ï¼Œä¸€ä¸ªæ€»æ˜¯è¯´è°ï¼Œä¸€ä¸ªæœ‰æ—¶è¯´çœŸè¯æœ‰æ—¶è¯´è°ã€‚ä½ åªèƒ½é—®å…¶ä¸­ä¸€ä¸ªäººä¸€ä¸ªé—®é¢˜ï¼Œå¦‚ä½•ç¡®å®šå“ªä¸ªæ˜¯è¯´çœŸè¯çš„äººï¼Ÿ\",\n",
    "            \"metrics\": [\"logic\", \"complexity\"]\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    # å­˜å‚¨æ‰€æœ‰æ¨¡å‹çš„æ‰€æœ‰é—®é¢˜å›ç­”\n",
    "    # ç»“æ„: {model: {question_id: response}}\n",
    "    all_responses = {model: {} for model in models}\n",
    "    \n",
    "    # ===== æ ¸å¿ƒæ”¹å˜ï¼šå¤–å±‚å¾ªç¯æ¨¡å‹ï¼Œå†…å±‚å¾ªç¯é—®é¢˜ =====\n",
    "    # è¿™æ ·å¯ä»¥è®©æ¯ä¸ªæ¨¡å‹ä¿æŒåŠ è½½çŠ¶æ€ï¼Œå‡å°‘æ˜¾å­˜åˆ‡æ¢\n",
    "    for model_idx, model in enumerate(models):\n",
    "        model_name = model.replace(\"qwen3:\", \"Qwen3 \").replace(\"b\", \"B\")\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(f\"ğŸš€ å¼€å§‹æµ‹è¯• {model_name} ({model_idx+1}/{len(models)})\")\n",
    "        print(\"=\"*80)\n",
    "        print(f\"ğŸ’¡ æ­¤æ¨¡å‹å°†è¿ç»­å›ç­” {len(test_questions)} ä¸ªé—®é¢˜\")\n",
    "        print()\n",
    "        \n",
    "        # éå†æ‰€æœ‰é—®é¢˜\n",
    "        for q in test_questions:\n",
    "            print(f\"\\n{'â”€'*80}\")\n",
    "            print(f\"ğŸ“ [{model_name}] é—®é¢˜ {q['id']}/{len(test_questions)}: {q['category']}\")\n",
    "            print(f\"{'â”€'*80}\")\n",
    "            print(f\"â“ {q['question']}\")\n",
    "            print()\n",
    "            \n",
    "            messages = [{\"role\": \"user\", \"content\": q['question']}]\n",
    "            \n",
    "            try:\n",
    "                response = ollama_chat_qwen(\n",
    "                    messages=messages,\n",
    "                    model=model,\n",
    "                    temperature=0.7,\n",
    "                    debug=debug,\n",
    "                    enable_thinking=enable_thinking,\n",
    "                    show_thinking=show_thinking\n",
    "                )\n",
    "                \n",
    "                all_responses[model][q['id']] = {\n",
    "                    'question': q,\n",
    "                    'response': response,\n",
    "                    'success': not response.startswith(\"[é”™è¯¯\")\n",
    "                }\n",
    "                \n",
    "                # æ³¨æ„ï¼šæµå¼æ¨¡å¼ä¸‹ï¼Œå›ç­”å·²ç»åœ¨ ollama_chat_qwen ä¸­å®æ—¶æ‰“å°äº†\n",
    "                # è¿™é‡Œåªæ˜¾ç¤ºç»Ÿè®¡ä¿¡æ¯\n",
    "                print(f\"ğŸ“ å®Œæ•´é•¿åº¦: {len(response)} å­—ç¬¦\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                error_msg = f\"[é”™è¯¯: {str(e)}]\"\n",
    "                all_responses[model][q['id']] = {\n",
    "                    'question': q,\n",
    "                    'response': error_msg,\n",
    "                    'success': False\n",
    "                }\n",
    "                print(f\"âŒ {error_msg}\")\n",
    "            \n",
    "            print()\n",
    "            time.sleep(2)  # é—®é¢˜é—´çŸ­æš‚å»¶è¿Ÿ\n",
    "        \n",
    "        print(f\"\\nâœ… {model_name} å®Œæˆæ‰€æœ‰ {len(test_questions)} ä¸ªé—®é¢˜\")\n",
    "        print(f\"{'='*80}\")\n",
    "        \n",
    "        # æ¨¡å‹é—´å»¶è¿Ÿï¼Œç»™æ˜¾å­˜åˆ‡æ¢ç•™å‡ºæ—¶é—´\n",
    "        if model_idx < len(models) - 1:\n",
    "            print(f\"\\nâ³ å‡†å¤‡åˆ‡æ¢åˆ°ä¸‹ä¸€ä¸ªæ¨¡å‹ï¼Œç­‰å¾…5ç§’...\")\n",
    "            time.sleep(5)\n",
    "    \n",
    "    # æ‰“å°å¯¹æ¯”åˆ†æå’Œæ€»ç»“\n",
    "    print(\"\\n\\n\" + \"=\"*80)\n",
    "    print(\"ğŸ“Š è·¨æ¨¡å‹å¯¹æ¯”åˆ†æ\")\n",
    "    print(\"=\"*80)\n",
    "    print_comparison_analysis(all_responses, models, test_questions)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"ğŸ“ˆ å®éªŒæ€»ç»“\")\n",
    "    print(\"=\"*80)\n",
    "    print_summary(all_responses, models)\n",
    "    \n",
    "    return all_responses\n",
    "\n",
    "\n",
    "def print_comparison_analysis(all_responses, models, test_questions):\n",
    "    \"\"\"æ‰“å°æ¯ä¸ªé—®é¢˜çš„è·¨æ¨¡å‹å¯¹æ¯”åˆ†æ\"\"\"\n",
    "    \n",
    "    for q in test_questions:\n",
    "        print(f\"\\n{'â”€'*80}\")\n",
    "        print(f\"ğŸ“ é—®é¢˜ {q['id']}: {q['category']}\")\n",
    "        print(f\"{'â”€'*80}\")\n",
    "        \n",
    "        # æ”¶é›†è¯¥é—®é¢˜çš„æ‰€æœ‰æ¨¡å‹å›ç­”\n",
    "        question_responses = {}\n",
    "        for model in models:\n",
    "            if q['id'] in all_responses[model]:\n",
    "                question_responses[model] = all_responses[model][q['id']]['response']\n",
    "        \n",
    "        # åŸºç¡€ç»Ÿè®¡\n",
    "        print(\"ğŸ“Š å›ç­”é•¿åº¦å¯¹æ¯”:\")\n",
    "        for model in models:\n",
    "            if model in question_responses:\n",
    "                response = question_responses[model]\n",
    "                model_name = model.replace(\"qwen3:\", \"\").upper()\n",
    "                \n",
    "                if response.startswith(\"[é”™è¯¯\"):\n",
    "                    print(f\"   {model_name}: âŒ å¤±è´¥\")\n",
    "                else:\n",
    "                    print(f\"   {model_name}: {len(response)} å­—ç¬¦\")\n",
    "        \n",
    "        # ç‰¹å®šç±»åˆ«åˆ†æ\n",
    "        analyze_question_category(q, question_responses, models)\n",
    "\n",
    "\n",
    "def analyze_question_category(question, responses, models):\n",
    "    \"\"\"æ ¹æ®é—®é¢˜ç±»åˆ«è¿›è¡Œé’ˆå¯¹æ€§åˆ†æ\"\"\"\n",
    "    \n",
    "    category = question['category']\n",
    "    \n",
    "    if \"é€»è¾‘\" in category or \"æ¨ç†\" in category:\n",
    "        print(\"\\nğŸ§  é€»è¾‘æ€§æŒ‡æ ‡:\")\n",
    "        logic_words = [\"å› ä¸º\", \"æ‰€ä»¥\", \"å¦‚æœ\", \"é‚£ä¹ˆ\", \"æ¨ç†\", \"ç»“è®º\", \"å‰æ\", \"å¿…ç„¶\"]\n",
    "        for model in models:\n",
    "            if model in responses:\n",
    "                response = responses[model]\n",
    "                if not response.startswith(\"[é”™è¯¯\"):\n",
    "                    count = sum(1 for word in logic_words if word in response)\n",
    "                    model_name = model.replace(\"qwen3:\", \"\").upper()\n",
    "                    print(f\"   {model_name}: {count} ä¸ªé€»è¾‘è¯\")\n",
    "    \n",
    "    elif \"æ•°å­¦\" in category:\n",
    "        print(\"\\nğŸ”¢ æ•°å­¦å…ƒç´ æŒ‡æ ‡:\")\n",
    "        math_indicators = [\"è®¡ç®—\", \"å…¬å¼\", \"æ–¹ç¨‹\", \"å°æ—¶\", \"Ã·\", \"Ã—\", \"+\", \"-\", \"=\", \"é€Ÿåº¦\", \"æ•ˆç‡\"]\n",
    "        for model in models:\n",
    "            if model in responses:\n",
    "                response = responses[model]\n",
    "                if not response.startswith(\"[é”™è¯¯\"):\n",
    "                    count = sum(1 for word in math_indicators if word in response)\n",
    "                    model_name = model.replace(\"qwen3:\", \"\").upper()\n",
    "                    print(f\"   {model_name}: {count} ä¸ªæ•°å­¦å…ƒç´ \")\n",
    "    \n",
    "    elif \"çŸ¥è¯†\" in category:\n",
    "        print(\"\\nğŸ“š çŸ¥è¯†æ·±åº¦æŒ‡æ ‡:\")\n",
    "        knowledge_words = [\"åŸç†\", \"æœºåˆ¶\", \"åº”ç”¨\", \"æŠ€æœ¯\", \"ç°è±¡\", \"ç†è®º\", \"é‡å­\", \"çº ç¼ \"]\n",
    "        for model in models:\n",
    "            if model in responses:\n",
    "                response = responses[model]\n",
    "                if not response.startswith(\"[é”™è¯¯\"):\n",
    "                    count = sum(1 for word in knowledge_words if word in response)\n",
    "                    model_name = model.replace(\"qwen3:\", \"\").upper()\n",
    "                    print(f\"   {model_name}: {count} ä¸ªä¸“ä¸šæœ¯è¯­\")\n",
    "    \n",
    "    elif \"ä»£ç \" in category:\n",
    "        print(\"\\nğŸ’» ä»£ç è´¨é‡æŒ‡æ ‡:\")\n",
    "        code_elements = [\"def\", \"return\", \"if\", \"for\", \"while\", \"try\", \"except\", \"import\", \"dict\"]\n",
    "        for model in models:\n",
    "            if model in responses:\n",
    "                response = responses[model]\n",
    "                if not response.startswith(\"[é”™è¯¯\"):\n",
    "                    count = sum(1 for elem in code_elements if elem in response)\n",
    "                    has_code = \"```\" in response or \"def \" in response\n",
    "                    model_name = model.replace(\"qwen3:\", \"\").upper()\n",
    "                    print(f\"   {model_name}: {count} ä¸ªä»£ç å…ƒç´ , ä»£ç å—: {'æœ‰' if has_code else 'æ— '}\")\n",
    "    \n",
    "    elif \"åˆ›æ„\" in category:\n",
    "        print(\"\\nâœ¨ åˆ›æ„æ€§æŒ‡æ ‡:\")\n",
    "        creative_words = [\"çªç„¶\", \"æ„å¤–\", \"å‘ç°\", \"æƒŠè®¶\", \"å¥‡å¦™\", \"ç¥ç§˜\", \"æ—¶é—´\", \"ç©¿è¶Š\"]\n",
    "        for model in models:\n",
    "            if model in responses:\n",
    "                response = responses[model]\n",
    "                if not response.startswith(\"[é”™è¯¯\"):\n",
    "                    count = sum(1 for word in creative_words if word in response)\n",
    "                    model_name = model.replace(\"qwen3:\", \"\").upper()\n",
    "                    print(f\"   {model_name}: {count} ä¸ªåˆ›æ„è¯æ±‡\")\n",
    "    \n",
    "    elif \"ä¼¦ç†\" in category or \"æŠ½è±¡\" in category:\n",
    "        print(\"\\nğŸ¤” æ€ç»´æ·±åº¦æŒ‡æ ‡:\")\n",
    "        depth_words = [\"ä½†æ˜¯\", \"ç„¶è€Œ\", \"è€ƒè™‘\", \"å¹³è¡¡\", \"é“å¾·\", \"ä¼¦ç†\", \"å“²å­¦\", \"è§‚ç‚¹\", \"è§’åº¦\"]\n",
    "        for model in models:\n",
    "            if model in responses:\n",
    "                response = responses[model]\n",
    "                if not response.startswith(\"[é”™è¯¯\"):\n",
    "                    count = sum(1 for word in depth_words if word in response)\n",
    "                    model_name = model.replace(\"qwen3:\", \"\").upper()\n",
    "                    print(f\"   {model_name}: {count} ä¸ªæ·±åº¦æ€è€ƒè¯\")\n",
    "\n",
    "\n",
    "def print_summary(all_responses, models):\n",
    "    \"\"\"æ‰“å°å®éªŒæ€»ç»“ç»Ÿè®¡\"\"\"\n",
    "    \n",
    "    print(\"\\nğŸ¯ æ•´ä½“è¡¨ç°å¯¹æ¯”:\")\n",
    "    \n",
    "    for model in models:\n",
    "        model_name = model.replace(\"qwen3:\", \"Qwen3 \").replace(\"b\", \"B\")\n",
    "        \n",
    "        success_count = 0\n",
    "        total_length = 0\n",
    "        \n",
    "        for q_id, data in all_responses[model].items():\n",
    "            if data['success']:\n",
    "                success_count += 1\n",
    "                total_length += len(data['response'])\n",
    "        \n",
    "        total_questions = len(all_responses[model])\n",
    "        avg_length = total_length // success_count if success_count > 0 else 0\n",
    "        success_rate = (success_count / total_questions) * 100 if total_questions > 0 else 0\n",
    "        \n",
    "        print(f\"\\n{model_name}:\")\n",
    "        print(f\"  âœ“ æˆåŠŸç‡: {success_count}/{total_questions} ({success_rate:.1f}%)\")\n",
    "        print(f\"  ğŸ“ å¹³å‡å›ç­”é•¿åº¦: {avg_length} å­—ç¬¦\")\n",
    "        \n",
    "        if success_count > 0:\n",
    "            # æ‰¾å‡ºæœ€é•¿å’Œæœ€çŸ­çš„å›ç­”\n",
    "            lengths = [len(data['response']) for data in all_responses[model].values() if data['success']]\n",
    "            print(f\"  ğŸ“Š å›ç­”é•¿åº¦èŒƒå›´: {min(lengths)} - {max(lengths)} å­—ç¬¦\")\n",
    "    \n",
    "    print(\"\\nğŸ’¡ ä¼˜åŒ–æ•ˆæœ:\")\n",
    "    print(f\"  âš¡ æ¨¡å‹åˆ‡æ¢æ¬¡æ•°: {len(models) - 1} æ¬¡ï¼ˆç›¸æ¯”é—®é¢˜å¾ªç¯å‡å°‘äº† {(len(all_responses[models[0]]) - 1) * (len(models) - 1)} æ¬¡ï¼‰\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"âœ… å®éªŒå®Œæˆï¼\")\n",
    "    print(\"=\"*80)\n",
    "\n",
    "\n",
    "# é»˜è®¤è¿è¡Œå®éªŒ\n",
    "print(\"ğŸš€ å‡†å¤‡è¿è¡Œå®éªŒ...\")\n",
    "print(\"ğŸ’¡ æç¤º: ä½¿ç”¨ experiment_8b_vs_14b_intelligence(debug=True) æŸ¥çœ‹è¯¦ç»†è°ƒè¯•ä¿¡æ¯\")\n",
    "print(\"ğŸ’¡ æ€è€ƒæ¨¡å¼: experiment_8b_vs_14b_intelligence(enable_thinking=True, show_thinking=True)\")\n",
    "#TODO ç”¨l4çš„æ—¶å€™è§£é™¤æ³¨é‡Š: æ·»åŠ 32Bæ¨¡å‹\n",
    "# print(\"ğŸ’¡ L4 GPU: ä½¿ç”¨ experiment_8b_vs_14b_intelligence(models=['qwen3:8b', 'qwen3:14b', 'qwen3:32b'])\")\n",
    "print()\n",
    "\n",
    "# æ‰§è¡Œå®éªŒ\n",
    "experiment_8b_vs_14b_intelligence()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
